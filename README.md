# Optimizing an ML Pipeline in Azure

## Overview
This project is part of the Udacity Azure ML Nanodegree.
In this project, we build and optimize an Azure ML pipeline using the Python SDK and a provided Scikit-learn model.
This model is then compared to an Azure AutoML run.

## Summary
**Problem Statement:** This dataset contains data about direct marketing campaigns of a Portuguese banking institution. This data contains customer's personal information such as age, education, job, marital status, took loan or not etc. We seek to predict next marketing campaign stratigies from the last marketing campaign of the bank. The prediction column contains whether the customer is subscribed to a term deposit or not.

**Solution:** The aim of this project is to use the banking dataset for two different approach and then compare and analyze the best result between these two approach.
In the first approach the optimization was based on using hyperdrive for hyper parameter tuning for Scikit-learn Logistic Regression. Here I got an accuracy of 0.90895.
In the second approach I used AutoML. The best model selected by AutoML was VotingEnsemble with an accuracy of 0.91697 making it the best approach here.

![bestmodel_autoML](https://user-images.githubusercontent.com/27814345/113926070-5f8aed80-9809-11eb-9e06-bf850d45e435.png)


## Scikit-learn Pipeline
**Pipeline architecture:** ![image](https://user-images.githubusercontent.com/27814345/114019050-943f8900-988b-11eb-9a87-134d4bb3aa7b.png)
Here, Scikit-learn Logistric Regression algorithm is used along with hyperDrive for hyperparameter tuning.

**Data:** At first in a training script I created TabularDataset using TabularDatasetFactory. The data source can be found here:   https://automlsamplenotebookdata.blob.core.windows.net/automl-sample-notebook-data/bankmarketing_train.csv .  
Next, the data was fed to a defined _clean_data_ function which removed null values, irrelevant columns and transformed the data using hot encoding. 
After data cleaning I splitted the data into train and test set.

**Hyperparameter tuning:** Two hyperparamters were used for tuning.  
                          •	Hyperparameter C, which is the inverse of regularization strength.   
                          •	Hyperparameter max_iter, which is the maximum number of iteration to converge.  
                          
**Classification Algorithm:** Logistic Regression algorithm from Scikit-learn was used for model training.

**What are the benefits of the parameter sampler you chose?**  
 Random Sampling is used as the parameter sampler in  which parameter values are chosen randomly from a set of discrete values or a distribution over a continuous range. So it   allows the search space to include both discrete and continuous hyperparameters. It supports the early termination of low-performance runs. Also, as randomness governs the selection process, each value has an equal probability of selection.
 
**What are the benefits of the early stopping policy you chose?**  
The early stopping policy chosen here is Bandit policy. Bandit policy is based on slack factor/slack amount and evaluation interval. Bandit ends runs when the primary metric isn't within the specified slack factor/slack amount of the most successful run.This policy terminates jobs that are not performing well and are not likely to yield a good model in the end based on the slack value. This early termination improves computational efficiency.

![image](https://user-images.githubusercontent.com/27814345/114164938-82242000-9949-11eb-9a01-1cf05d7b802c.png)

## AutoML  
The primary metric for selecting the best model was Accuracy. The best model chosen was the Voting Ensamble having accuracy score 0.91697. In this clasifer instead of using only one classifier but a set of classifiers and then combine their predictions for the classification of unseen instances using some form of voting. 
The hyperparameters generated by the AutoML are:  
•	**Ensemble weights**: [0.4666666666666667, 0.13333333333333333, 0.06666666666666667, 0.06666666666666667, 0.13333333333333333, 0.06666666666666667, 0.06666666666666667]))]  
  Ensemble weight is an extension of a model averaging ensemble where the contribution of each member to the final prediction is weighted by the performance of the mode.  
•	**min_impurity_split**=None  
  min_impurity_split parameter is used to control the tree based on impurity values. It sets a threshold on gini. A node needs to have a gini value that is more than the         threshold  to be further splitted.  
•	**min_samples_split**: 0.01  
  min_samples_split specifies the minimum number of samples required to split an internal node.  
•	**min_weight_fraction_leaf**: 0.0
  min_weight_fraction_leaf is the fraction of the input samples required to be at a leaf node where weights are determined by sample_weight, this is a way to deal with class imbalance.  
•	**n_jobs**: 1  
  n_jobs is an integer, specifying the maximum number of concurrently running jobs.  
•	**tol**: 0.0001  
  tol is erance for the optimization. When the score is not improving by at least tol for two consecutive iterations, unless `learning_rate` is set to 'adaptive', convergence is considered to be reached and training stops.  

![image](https://user-images.githubusercontent.com/27814345/114171452-ee0a8680-9951-11eb-8883-73bf1380182d.png)

## Pipeline comparison  
**Compare the two models and their performance. What are the differences in accuracy? In architecture? If there was a difference, why do you think there was one?**  
There was not a big difference in terms of accuracy between the two models. But to conclude AutoML model gives best accuracy of 0.91697 (with VotingEnsembleClassifier), while the hyperdrive driven Logistic Regression model had a slightly lower score of 0.90895. One of the main reason of difference can be that, Voting Ensamble is an ensemble of different classifiers and their prediction is combined for the final prediction. Thus it has an edge over the classic Logistic Regression model.

## Future work  
**What are some areas of improvement for future experiments? Why might these improvements help the model?**  
  •	We can spend more time on EDA to get a better picture of the dataset, so that data cleaning and transformation can be improved. This will result in better training and eventually better prediction by the model.  
  •	We can try other hyper parameters for tuning in case of Scikit-learn Logistic Regression model. This will help us to compare different results and find out the best hyper parameters to be tuned.  
  •	Also, to compare with AutoML we can try other models rather than Logistic Regression for the hyperdrive driven approach and compare the results to research and get more suitable models.  
  •	We can try out different parameters for hyperdrive config as well as for AutoML config. Trying out different parameters will fetch different result and comparing and analyzing those results will help to choose best config parameters for this particular usecase.  

## Proof of cluster clean up
![image](https://user-images.githubusercontent.com/27814345/114174961-0630d480-9957-11eb-8828-cb2be6c856b0.png)
